{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#                           Wrangle OpenStreetMap Data (Houston Area):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Data Wrangling is one of the most important phases in Data Sceince. Sources say that data wrangling constitute to 70 percent of Data analysis work. It is a process of gathering, extracting, cleaning and storing our data [1]. Most of the data avaliable today is in complex formats. Those formats include data from social networking sites, video streaming sites etc.. in XML,JSON or many standard formats. Data wrangling begins with gathering of data from any of the sources. In this project, the data is being gathered from OpenStreetMap. OpenStreetMap distributes free geographic data of the world [2]. It provides the map data in many different formats. This project uses different data wrangling techniques to check the quality of OSM data.  \n",
    "\n",
    "Programming Language: Python\n",
    "\n",
    "Database: Mongo DB\n",
    "\n",
    "Modules used: Cleaning.py\n",
    "\n",
    "#### Download link: https://mapzen.com/data/metro-extracts/metro/houston_texas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "                                           ##################\n",
    "                                           #  Project Flow: #\n",
    "                                           ##################\n",
    "        \n",
    "        1) Auditing:\n",
    "            1.1) Loading the OSM file.\n",
    "            1.2) Study of the file structure(tags,attributes etc...)\n",
    "            1.3) Auditing different tags to identify consistent inconsistencies.\n",
    "        \n",
    "        2) Cleaning:\n",
    "            2.1) Defining the cleaning function for each attribute\n",
    "            2.2) Shaping the element\n",
    "            2.3) Writing the shaped element to JSON file.\n",
    "            \n",
    "        3) Mongo DB:\n",
    "            3.1) Importing the JSON file to Mongo DB\n",
    "            3.2) Validating the counts\n",
    "            3.3) Statistical overview of dataset using DB queries\n",
    "        \n",
    "        4) Ideas for additional improvements\n",
    "            \n",
    "'''\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import re\n",
    "import phonenumbers\n",
    "import codecs\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Auditing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of houston_texas.osm file in MB:\n",
      "691\n"
     ]
    }
   ],
   "source": [
    "'''houston_texas.osm file is downloaded from Mapzen metro extracts.\n",
    "   The downloaded file is in compressed format (.osm.bz2). We can extract\n",
    "   the actual file using win-rar extarctor. The file will be very huge. \n",
    "   The OSM file of huston is approx 700 MB!'''\n",
    "\n",
    "print (\"Size of houston_texas.osm file in MB:\")\n",
    "print (os.path.getsize(\"houston_texas.osm\")/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "osm_file = open(\"houston_texas.osm\", \"r\")\n",
    "'''The data in OSM file is organized in XML format. \n",
    "   It has a root node and many child nodes. Python provides us many\n",
    "   ways to parse an XML file. Since the file is huge there is a need\n",
    "   parse the file iteratively. By parsing iteratively we load one node\n",
    "   into memory for each iteration. To identify the elements that are to\n",
    "   be audited and cleaned we should be aware of different types of tags in \n",
    "   the file.'''\n",
    "\n",
    "all_tags = {}\n",
    "\n",
    "for _,element in ET.iterparse(osm_file):\n",
    "    if element.tag in all_tags:\n",
    "        all_tags[element.tag] += 1\n",
    "    else:\n",
    "        all_tags[element.tag] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 27113,\n",
      " 'nd': 3634961,\n",
      " 'node': 3039649,\n",
      " 'osm': 1,\n",
      " 'relation': 2467,\n",
      " 'tag': 2089952,\n",
      " 'way': 368288}\n"
     ]
    }
   ],
   "source": [
    "'''All the tags of OSM file are displayed below. OSM has three main data structures [3]\n",
    "    1) Node\n",
    "    2) Way\n",
    "    3) Relation\n",
    "    \n",
    "    Each tag describes a geographic attribute of the feature being shown by that specific node, \n",
    "    way or relation.\n",
    "    \n",
    "    A node is one of the core elements in the OpenStreetMap data model. It consists of a single point in \n",
    "    space defined by its latitude, longitude and node id.\n",
    "    \n",
    "    A way is an ordered list of nodes which normally also has at least one tag or is included within a \n",
    "    Relation. \n",
    "    \n",
    "    A relation consists of one or more tags and also an ordered list of one or more nodes, ways and/or \n",
    "    relations as members which is used to define logical or geographic relationships between other \n",
    "    elements.\n",
    "    \n",
    "    '''\n",
    "\n",
    "pprint.pprint (all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Houston metro extract has: 3039649 nodes and 368288 ways\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  This project audits and cleans only two types of tags(nodes,ways) and their attributes. With the help \n",
    "  of classroom casestudy the following steps are considered for auditing and cleaning:\n",
    "\n",
    " - Process only 2 types of top level tags: \"node\" and \"way\n",
    " \n",
    " - All attributes of \"node\" and \"way\" should be turned into regular key/value pairs, except:\n",
    "     - attributes in the CREATED array should be added under a key \"created\"\n",
    "     - attributes for latitude and longitude should be added to a \"pos\" array,\n",
    "       for use in geospacial indexing. Make sure the values inside \"pos\" array are floats\n",
    "       and not strings.\n",
    "       \n",
    " - If the second level tag \"k\" value contains problematic characters, it should be ignored.\n",
    " \n",
    " - If the second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
    " \n",
    " - The value of addr:street should be audited and the unexpected street types should be cleaned to an\n",
    "   appropriate ones in the expected list provided by Houston city council[4]. The street name may appear\n",
    "   in both the node and way tags.\n",
    "   For example: St.  => Street\n",
    "                Blvd => Boulevard\n",
    "   \n",
    " - If the second level tag \"k\" value does not start with \"addr:\", but contains \":\", it can be\n",
    "   processed any way. \n",
    "    \n",
    " - If there is a second \":\" that separates the type/direction of a street,\n",
    "   the tag should be ignored.(Only for attributes of type 'addr')\n",
    "   \n",
    " - The value of addr:city should be audited and the unexpected city names should be cleaned to \n",
    "   exact city name.\n",
    "   For example: \"Pearland, TX\" => \"Pearland\"\n",
    "   \n",
    " - The value of addr:postcode should be audited and the unexpected post codes should be cleaned to \n",
    "   standard format.\n",
    "   For example: \"TX 77009\"   => \"77009\"\n",
    "                \"77340-3124\" => \"77340\"  \n",
    "            \n",
    " - If there is a second \":\", it should be replaced with \"_\" and added as key value pairs\n",
    "\n",
    " - The value of phone should be audited and converted to standard format. There are many \n",
    "   standard formats for phone number but I would like to store the number as one format in\n",
    "   my DB. The standard format would be (XXX) XXX-XXXX.\n",
    "   For example: = +1 281-776-0143 => (281) 776-0143\n",
    "  '''\n",
    "print \"The Houston metro extract has: %d nodes and %d ways\" %(all_tags[\"node\"],all_tags[\"way\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Defining regular expressions:\n",
    "street = re.compile(r'^(addr:street$)')\n",
    "\n",
    "city = re.compile(r'^(addr:city$)')\n",
    "\n",
    "postcode = re.compile(r'^(addr:postcode$)')\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "#The above regular expression captures the last part of the street name\n",
    "#For example: street_type_re.search(Hillcroft Rd.) => Rd.\n",
    "\n",
    "city_re = re.compile(r'TX$', re.IGNORECASE)\n",
    "\n",
    "zero_one_colon = re.compile(r'^(\\w+:?\\w*$)')\n",
    "\n",
    "zip_code1_re = re.compile(r'(-\\d*$)')\n",
    "zip_code2_re = re.compile(r'(^TX)',re.IGNORECASE)\n",
    "\n",
    "phone_start_re = re.compile(r'^(\\(|\\+1|[2-9])')\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "'''\n",
    "The approved list is provided by Houston city council. It is avalible in appendix III of [4]\n",
    "'''\n",
    "\n",
    "Houston_approved_st_types = [\"Avenue\",\"Boulevard\",\"Bridge\",\"Bypass\",\"Circle\",\"Court\",\"Crossing\",\n",
    "                             \"Crossroad\",\"Drive\",\"Expressway\",\"Fork\",\"Freeway\",\"Freeway\",\"Highway\",\n",
    "                             \"Lane\",\"Loop\",\"Motorway\",\"Oval\",\"Parkway\",\"Passage\",\"Path\",\"Place\",\n",
    "                             \"Road\",\"Street\",\"Throughway\",\"Trail\",\"Tunnel\",\"Way\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'Auditing the necessary attributes:'\n",
    "street_types = defaultdict(set)\n",
    "post_code_set = set()\n",
    "city_set = set()\n",
    "phone_set = set()\n",
    "\n",
    "def audit_osm(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    for _, element in ET.iterparse(osm_file):\n",
    "        if element.tag == \"way\" or element.tag == \"node\":\n",
    "            for tag in element.iter(\"tag\"):\n",
    "                if re.search(street,tag.attrib[\"k\"]):\n",
    "                    m = street_type_re.search(tag.attrib[\"v\"])\n",
    "                    if m:\n",
    "                        street_type = m.group()\n",
    "                        if street_type not in Houston_approved_st_types:\n",
    "                            street_types[street_type].add(tag.attrib[\"v\"])\n",
    "                elif re.search(city,tag.attrib[\"k\"]):\n",
    "                    city_set.add(tag.attrib[\"v\"])\n",
    "                elif re.search(postcode,tag.attrib[\"k\"]):\n",
    "                    post_code_set.add(tag.attrib[\"v\"])\n",
    "                elif(tag.attrib[\"k\"]==\"phone\"):\n",
    "                    phone_set.add(tag.attrib[\"v\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_in = \"houston_texas.osm\"\n",
    "audit_osm(file_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1142', 'Walk', 'Ridge', 'FM646', 'Lake', 'F-3', 'Rd', '130', 'Bailey', 'Oaks', 'TX-332', 'Texas', '1774', 'Mews', 'Business', '90A', 'Cypress', 'Dallas', '6475', 'Mall', '1464', '285', 'Fuzzel', '1462', 'Maroneal', '1092', 'Montrose', 'L', 'Pkwy', '121', 'T', 'one', '701', '59', 'Westheimer', 'Durham', 'Broadway', '1663', '925', '290', 'S.', '146', 'Beechnut', '596', 'Driscoll', 'West', '316', '270', 'Stree', '77027', 'street', '227A', '110', '1093', '1488', '87', 'Road)', 'Run', 'Hillcroft', 'Park', '77598', 'Isle', '90a', '521', '362', '529', 'Elm', '309', 'C', '303', 'Ave.', 'G', '300', 'Plaza', 'O', 'Plaze', 'Felipe', 'S', 'Fwy', '242', '103', '249', 'HIGHWAY', '105', 'Square', '1640', 'Point', 'MacGregor', 'Westhimer', '36', 'Speedway', '518', '646', 'Welford', 'Hidalgo', 'Es', '240', '9k', 'T2008', 'Blossom', '2920', 'St.', 'Rock', '575', '332', 'Richmond', '65', 'A-527', 'North', 'Crosstimbers', '704', '180', 'FM517', '650', '6', 'Blvd.', '502', 'B', 'N,', '1/2', 'F', '185', '565', 'St', 'R', '90', '160', '94', '302-B', '10', 'Marketplace', 'Rd.', 'ST', 'I-45', 'Frwy', 'Ave', '2100', 'Ct', '150', 'Caroline', 'Ln', 'Kempwood', 'DRIVE', 'fannin', 'Graustark', 'Riverway', 'blvd', 'Valley', 'Real', 'East', '7A', '45', '1960', 'Americas', 'TX-6', '77096', 'Trace', 'F463', 'Dr', 'A', '200', 'Expy', 'Knolls', 'E', 'Route', 'Q', 'Gulfcrest', 'Larchmont', 'N', 'South', 'Houston', '800', 'TX', '1764', '940', 'Terrace', 'Village', 'Blvd', 'Row', '359', '125']\n"
     ]
    }
   ],
   "source": [
    "print(street_types.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['KATY', 'La Porte', 'katy', 'El Lago', 'The Woodlands', 'Jersey Village', 'Porter', 'Klein', 'Nassau Bay', 'Cypress', 'Wallis', 'Hockley', 'Angleton,TX', 'Conroe', 'DEER PARK', 'La Marque', 'Crystal Beach', 'Spring, TX', 'Lake Jackson, TX', 'Webster', 'Katy, TX', 'Tomball, Tx', 'Katy', 'Fresno', 'Laks Jackson', 'TEXAS CITY', 'League City, TX', 'Winnie', 'clear lake shores', 'Deer Park', 'Houston, TX', 'Friendswood, TX', 'Pearland', 'Dickinson', 'Liberty', 'Fulshear', 'Little York', 'Baytown', 'Beasley', 'HOUSTON', 'Hempstead', 'Angleton, TX', 'Rosharon', 'Santa Fe, TX', 'Tomball', 'West Columbia, TX', 'Atascocita', 'Woodlands', 'San Leon', 'Seabrook', 'Hedwig Village', 'Clear Lake Shores', 'Plantersville', 'Sugar Land', 'New Caney', 'Alvin', 'Sealy', 'Rosenberg', 'Friendswood', 'Santa Fe', 'houston', 'Alvin, TX', 'West University', 'MAGNOLIA', 'Houston, Texas', 'Magnolia', 'Bellaire', 'Richmond', 'Pasadena', 'Humble, TX', 'Cypress, TX', 'Pasadena, TX', 'Kemah', 'Shenandoah', 'West University Place', 'Missouri City, TX', 'League City', 'Bay City, TX', 'Channelview', 'Kingwood', 'Humble', 'South Houston', 'Galveston Island', 'Todd Mission', 'Brazoria', 'Crosby', 'Kingwood, TX', 'Angleton', 'Sugar Land, TX', 'Needville', 'Brookshire', 'Sugarland', 'Lake Jackson', 'Texas City', 'Bay City', 'Bellaire, TX', 'LAKE JACKSON', 'The Woodlands, TX', 'Kendleton', 'Dickenson', 'Missouri City', 'Stafford', 'Dickinson, Tx', 'Mont Belvieu', 'Fort Bend', 'Meadows Place', 'Freeport', 'Clute', 'Navasota', '77386', 'Waller', 'Houson', 'Galveston', 'Houston', 'Pearland, TX', 'LaMarque', 'Spring', 'Huffman'])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "   After auditing the city names, there are few inconsistent city names which are appended with \", Tx\". \n",
    "   Those cities can be updated and made consistent just by having city name alone.\n",
    "   For example: 'Angleton,TX' => 'Angleton'\n",
    "   \n",
    "'''\n",
    "print city_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['77047', '77007-2121', '77045', '77044', '77043', '77042', '77041', '77040', '77575', '77573', '77571', '77451', '77379', '77375', '77377', '77373', '77477', '77474', '77017', '77471', '77049', '77478', '77479', '77083', '73032', '77027-6850', '77584-', '77338', '77568', '77051', '77357', '77053', '77054', '77055', '77056', '77007-2113', '77058', '77059', 'TX 77009', '77565', '77566', '77388', 'TX 77494', '77365', 'TX 77086', '77363', '74404', '77441', '77447', '773867386', '77449', '77590', '77389', '77025-9998', '77025', '77024', '77027', '77021', '77020', '77023', '77022', '77515', '77511', '77510', '77355', '77354', '77530', '77591', '77459', '77450', '77598', '77019-1999', '77204', '77345', '77346', '77042-9998', '77036', '77037', '77034', '77035', '77032', '77030', '77031', '77506', '77504', '77505', '77502', '77503', '77038', '77039', '77429', '77587', '77584', '77583', '77580', '77581', 'Weslayan Street', '77423', '77422', '77665', 'tx 77042', '77498', '77089', '77088', '77054-1921', '77336', '77339', '77082', '77081', '77080', '77087', '77086', '77085', '77084', '77003', '77002', '77539', '77007', '77006', '77005', '77004', '77532', '77009', '77008', '77536', '77535', '77339-1510', '77005-1890', '77433', '75057', '773345', '77802', '77024-8022', '77586', '77520', '77521', '77489', '77523', '77486', '77485', '77407', '77406', '77018', '77019', '77401', '77014', '77015', '77016', '77246', '77010', '77011', '77012', '77013', '77068', '77396', '77478-', '77531', '77036-3590', '77551', '77550', '77553', '77555', '77554', '77492', '77493', '77494', '77069', '88581', '77414', '77417', '77061', '77060', '77063', '77062', '77065', '77064', '77067', '77066', '77007-2112', '77029', '77650', '77057', '77384-5317', '77028', '77094', '77095', '77096', '77090', '77091', '77092', '77093', '77098', '77099', '77541', '77546', '77545', '7-', '77459-', '77380', '77381', '77382', '77079', '77384', '77385', '77386', '770764', '77072', '77073', '77070', '77071', '77076', '77077', '77074', '77075', '77461', 'TX 77005', '77469', '77468', '77046', '77868', 'TX 77042', '77077-9998', 'TX 77043'])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    After auditing the postal codes, there are few inconsistent codes which begin with 'TX'. \n",
    "    Those postal codes can be updated to one consistent format. The second type of cleaning \n",
    "    can be done on nine digit postal codes. Nide digits can be converted to a five digit code.\n",
    "\n",
    "    For example: a) \"TX 77086\"    => \"77086\"\n",
    "                 b) \"77025-9998\"  => \"77025\" \n",
    "'''\n",
    "\n",
    "print (post_code_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    The process_map shapes the element using all the update functions defined in Cleaning.py script.\n",
    "    also writes the shaped elemnt to JSON file. For more details, see Cleaning.py\n",
    "'''\n",
    "from Cleaning import process_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_map(file_in, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### MongoDB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The JSON file is imported into mongo DB from cmd prompt. I tried importing using pymongo but faced the similar\n",
    "which most of the studentd faced. Discussion forum saved my life and found that the best way to import is throgh cmd prompt.\n",
    "    \n",
    "    Command:\n",
    "    \n",
    "    mongoimport -d openstreetmap -c houston --file \"C:\\Users\\vemul\\Data_Analyst_Nano_Degree\\P3-Open_Street_Map_Data_\n",
    "    Wrangling-Python_MongoDB\\houston_texas.osm.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.openstreetmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'storageSize': 265359360.0, u'ok': 1.0, u'avgObjSize': 246.62568234095878, u'views': 0, u'db': u'openstreetmap', u'indexes': 1, u'objects': 3407937, u'collections': 1, u'numExtents': 0, u'dataSize': 840484788.0, u'indexSize': 34189312.0}\n"
     ]
    }
   ],
   "source": [
    "# Statistics of OSM collection:\n",
    "print db.command(\"dbstats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents inserted: 3407937\n",
      "Number of nodes inserted: 3039646\n",
      "Number of ways inserted: 368276\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Some fun with the counts:\n",
    "'''\n",
    "num_docs = db.houston.find().count()\n",
    "print \"Number of documents inserted:\", num_docs\n",
    "\n",
    "node_query = {\"type\":\"node\"}\n",
    "num_nodes = db.houston.find(node_query).count()\n",
    "print \"Number of nodes inserted:\", num_nodes\n",
    "\n",
    "way_query = {\"type\":\"way\"}\n",
    "num_ways = db.houston.find(way_query).count()\n",
    "print \"Number of ways inserted:\", num_ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    There is a count mismatch. We have document types other than node and way inserted into mongo DB. \n",
    "    We have 15 such documents\n",
    "'''\n",
    "pipeline = [\n",
    "            {\"$match\":{\"type\":{\"$ne\":\"node\"}}},\n",
    "            {\"$match\":{\"type\":{\"$ne\":\"way\"}}}      \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public\n",
      "building\n",
      "dance\n",
      "public\n",
      "oil\n",
      "outdoor\n",
      "gas\n",
      "gas\n",
      "gas\n",
      "gas\n",
      "civil\n",
      "Public\n",
      "public\n",
      "multipolygon\n",
      "multipolygon\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Print all the inconsistent types:\n",
    "'''\n",
    "for doc in db.houston.aggregate(pipeline):\n",
    "    print doc[\"type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### After researching those specific documets, I noticed that they are actually node or way types but they are having extra attribute called \"type\". So the exisiting type value (node or way) in the dictionary is repalced. As there are only 15 such documents I decided to leave as them as such without deleting or updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users who contributed to OSM-Houston area: 1648\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Number of unique users:\n",
    "'''\n",
    "users = db.houston.distinct('created.user')\n",
    "print \"Number of users who contributed to OSM-Houston area:\", len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def aggregate(pipeline):\n",
    "    return [doc for doc in db.houston.aggregate(pipeline,allowDiskUse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'woodpeck_fixbot', u'count': 567584},\n",
      " {u'_id': u'TexasNHD', u'count': 538419},\n",
      " {u'_id': u'afdreher', u'count': 478666},\n",
      " {u'_id': u'scottyc', u'count': 204770},\n",
      " {u'_id': u'cammace', u'count': 192874},\n",
      " {u'_id': u'claysmalley', u'count': 136311},\n",
      " {u'_id': u'brianboru', u'count': 117229},\n",
      " {u'_id': u'skquinn', u'count': 86202},\n",
      " {u'_id': u'RoadGeek_MD99', u'count': 81986},\n",
      " {u'_id': u'Memoire', u'count': 56656}]\n"
     ]
    }
   ],
   "source": [
    "# Few basic stats (Ones suggested in project requirements)\n",
    "'''\n",
    "    Top ten creators\n",
    "'''\n",
    "user_pipeline = [{\"$group\":{\"_id\": \"$created.user\",\n",
    "                          \"count\":{\"$sum\":1}}},\n",
    "                 {\"$sort\":{\"count\":-1}},\n",
    "                 {\"$limit\":10}\n",
    "                ]\n",
    "users = aggregate(user_pipeline)\n",
    "pprint.pprint(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Bing', u'count': 5926},\n",
      " {u'_id': u'USGS Geonames', u'count': 876},\n",
      " {u'_id': u'Yahoo', u'count': 589},\n",
      " {u'_id': u'bing', u'count': 300},\n",
      " {u'_id': u'Mapbox', u'count': 282},\n",
      " {u'_id': u'PGS', u'count': 221},\n",
      " {u'_id': u'Yahoo,TIGER', u'count': 160},\n",
      " {u'_id': u'http://www.epa.gov/enviro/geo_data.html', u'count': 80},\n",
      " {u'_id': u'TIGER/Line\\xae 2008 Place Shapefiles (http://www.census.gov/geo/www/tiger/)',\n",
      "  u'count': 70},\n",
      " {u'_id': u'ground', u'count': 59}]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Top ten sources\n",
    "'''\n",
    "source_pipeline = [{\"$match\":{\"source\":{\"$exists\":1}}},\n",
    "                   {\"$group\":{\"_id\":\"$source\",\n",
    "                             \"count\":{\"$sum\":1}}},\n",
    "                   {\"$sort\":{\"count\":-1}},\n",
    "                   {\"$limit\":10}\n",
    "                  ]\n",
    "sources = aggregate(source_pipeline)\n",
    "pprint.pprint(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'amenity': u'pharmacy', u'name': u'H-E-B Pharmacy'},\n",
      " {u'amenity': u'dentist', u'name': u'Portofino Dental'},\n",
      " {u'amenity': u'pharmacy', u'name': u'Randalls Pharmacy'},\n",
      " {u'amenity': u'restaurant', u'name': u\"Fleming's Steakhouse\"},\n",
      " {u'amenity': u'restaurant', u'name': u'Atsumi'},\n",
      " {u'amenity': u'veterinary', u'name': u'Windvale Pet Hospital'},\n",
      " {u'amenity': u'restaurant', u'name': u'Fogo de Ch\\xe3o Brazilian Steakhouse'},\n",
      " {u'amenity': u'place_of_worship', u'name': u'The Woodlands Methodist Church'},\n",
      " {u'amenity': u'cafe', u'name': u'Starbucks'},\n",
      " {u'amenity': u'bank', u'name': u'Citizens Bank'},\n",
      " {u'amenity': u'fast_food', u'name': u'Chick-fil-A'},\n",
      " {u'amenity': u'fast_food', u'name': u'Whataburger'},\n",
      " {u'amenity': u'restaurant', u'name': u'Sweet Tomatoes'},\n",
      " {u'amenity': u'school', u'name': u'Knox Junior High School'},\n",
      " {u'amenity': u'school', u'name': u'The Woodlands College Park High School'},\n",
      " {u'amenity': u'place_of_worship',\n",
      "  u'name': u'Northwoods Unitarian Universalist Church'},\n",
      " {u'amenity': u'fast_food', u'name': u'Chick-fil-A'},\n",
      " {u'amenity': u'fast_food', u'name': u'Taco Cabana'}]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Ameneties in The WoodLands city(My locality). I learnt how to use logical operator within \n",
    "    match operator [5].\n",
    "'''\n",
    "woodlands_pipeline = [{\"$match\":{\n",
    "                           \"$or\":[\n",
    "                                   {\"address.city\":{\"$eq\":\"The Woodlands\"}},\n",
    "                                   {\"address.city\":{\"$eq\":\"woodlands\"}} \n",
    "                                 ]}},\n",
    "                       {\"$match\":{\"amenity\":{\"$exists\":1}}},\n",
    "                       {\"$project\":{\"amenity\":\"$amenity\",\n",
    "                                  \"name\":\"$name\",\n",
    "                                   \"_id\":0}}\n",
    "                     ]\n",
    "woodlands_amenity = aggregate(woodlands_pipeline)\n",
    "pprint.pprint(woodlands_amenity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Houston', u'count': 1007},\n",
      " {u'_id': u'Kingwood', u'count': 96},\n",
      " {u'_id': u'Katy', u'count': 92},\n",
      " {u'_id': u'Sugar Land', u'count': 74},\n",
      " {u'_id': u'Tomball', u'count': 69}]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Top five areas in Houston with more number of amenities:\n",
    "'''\n",
    "city_amenities_pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}}},\n",
    "                           {\"$match\":{\"address.city\":{\"$exists\":1}}},\n",
    "                           {\"$group\":{\"_id\":\"$address.city\",\n",
    "                             \"count\":{\"$sum\":1}}},\n",
    "                           {\"$sort\":{\"count\":-1}},\n",
    "                           {\"$limit\":5}\n",
    "                          ]\n",
    "city_amenities = aggregate(city_amenities_pipeline)\n",
    "pprint.pprint(city_amenities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'name': u'Chinese Buffet'},\n",
      " {u'name': u'Grand Buffet'},\n",
      " {u'name': u\"P. F. Chang's China Bistro\"},\n",
      " {u'name': u'Chopsticks'},\n",
      " {u'name': u'Chinois Orient Bistro'},\n",
      " {u'name': u'Fairwan Hunan Restaurant'},\n",
      " {u'name': u'Wan Fu'},\n",
      " {u'name': u'Panda Express'},\n",
      " {u'name': u\"Hu's Garden\"},\n",
      " {u'name': u'Pei Wei'},\n",
      " {u'name': u'Hunan River Bistro'},\n",
      " {u'name': u'D Wok Express'},\n",
      " {u'name': u'Chinese Buffet'},\n",
      " {u'name': u'Yu Garden'},\n",
      " {u'name': u'China Stix'},\n",
      " {u'name': u'888 Chinese Restaurant'},\n",
      " {u'name': u\"Fang's Cafe\"},\n",
      " {u'name': u'Panda Express'},\n",
      " {u'name': u'Kim Son'},\n",
      " {u'name': u'Los Chinos Rico'},\n",
      " {u'name': u'Vstar'},\n",
      " {u'name': u\"Chang's Chinese\"},\n",
      " {u'name': u\"Hin's Garden\"},\n",
      " {u'name': u'Happy Lamp Restaurant '},\n",
      " {u'name': u'Panda Express'},\n",
      " {u'name': u'Cafe East Chinese Restaurant and Buffet'},\n",
      " {u'name': u'Oriental Gardens Chinese Restaurant'},\n",
      " {u'name': u'Hunan Garden Restaurant'},\n",
      " {u'name': u'East Star Chinese Buffet'},\n",
      " {u'name': u'Szechuan Garden'},\n",
      " {u'name': u\"Wong's Chef\"},\n",
      " {u'name': u'888 Chinese Restaurant'},\n",
      " {u'name': u'MaTu Chinese Restaurant'},\n",
      " {u'name': u'Panda Express'},\n",
      " {u'name': u'China Garden Restaurant'},\n",
      " {u'name': u'Chinese Star Restaurant'},\n",
      " {u'name': u'Panda Express'},\n",
      " {u'name': u'888 Chinese Restaurant'},\n",
      " {u'name': u'Panda Express'},\n",
      " {u'name': u'Panda Express'},\n",
      " {u'name': u'Bamboo House'},\n",
      " {u'name': u'Hunan Hut'}]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Eataries with Chinese Cuisine:\n",
    "'''\n",
    "chinese_cuisine_pipeline = [{\"$match\":{\n",
    "                            \"$or\":[\n",
    "                                   {\"amenity\":{\"$eq\":\"restaurant\"}},\n",
    "                                   {\"amenity\":{\"$eq\":\"fast_food\"}} \n",
    "                                 ]}},\n",
    "                            {\"$match\":{\"cuisine\":{\"$eq\":\"chinese\"}}},\n",
    "                            {\"$project\":{\n",
    "                                  \"name\":\"$name\",\n",
    "                                   \"_id\":0}}\n",
    "                           ]\n",
    "chinese_cuisine = aggregate(chinese_cuisine_pipeline)\n",
    "pprint.pprint(chinese_cuisine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'3759210966', u'count': 14}]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    What is the most referred node in Houston city? Bar? Place_of_Worship? Lets check it out!\n",
    "'''\n",
    "node_ref_pipeline = [{\"$match\":{\"type\":{\"$eq\":\"way\"}}},\n",
    "                     {\"$unwind\":\"$node_refs\"},\n",
    "                     {\"$group\":{\"_id\":\"$node_refs\",\n",
    "                             \"count\":{\"$sum\":1}}},\n",
    "                     {\"$sort\":{\"count\":-1}},\n",
    "                     {\"$limit\":1}\n",
    "                    ]\n",
    "node_ref = aggregate(node_ref_pipeline)\n",
    "pprint.pprint(node_ref)\n",
    "\n",
    "# Initially the group statemnt exceded 100MB of data and the query failed. Upon further researching I learnt about\n",
    "# allowDiskUse=True option in aggrgation framework [6]. I updated the aggregate funnction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': ObjectId('595b0177a7c5d286d0f6c9ee'),\n",
      "  u'created': {u'changeset': u'34249805',\n",
      "               u'timestamp': u'2015-09-25T18:18:05Z',\n",
      "               u'uid': u'3119079',\n",
      "               u'user': u'cammace',\n",
      "               u'version': u'1'},\n",
      "  u'id': u'3759210966',\n",
      "  u'pos': [29.7214822, -95.3403131],\n",
      "  u'type': u'node'}]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Extracting the most referenced node details:\n",
    "'''\n",
    "ref = [{\"$match\":{\"id\":{\"$eq\":node_ref[0][\"_id\"]}}}\n",
    "      ]\n",
    "n = aggregate(ref)\n",
    "pprint.pprint(n)\n",
    "\n",
    "# Unfortunately we do not have any deatils of the most referenced node except the position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'77545']\n",
      "[u'77363']\n",
      "[u'77012']\n",
      "[u'77028']\n",
      "[u'77093']\n",
      "[u'77061']\n",
      "[u'77520']\n",
      "[u'77081']\n",
      "[u'77573']\n",
      "[u'77486']\n",
      "[u'77590']\n",
      "[u'77056']\n",
      "[u'77066']\n",
      "[u'77385']\n",
      "[u'77023']\n",
      "[u'77450']\n",
      "[u'77006']\n",
      "[u'77554']\n",
      "[u'77030']\n",
      "[u'77506']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Time to validate our update_zipcode function:\n",
    "'''\n",
    "zipcode_pipeline = [{\"$match\":{\"address.postcode\":{\"$exists\":1}}},\n",
    "                    {\"$group\":{\"_id\":\"$address.postcode\"}}\n",
    "                   ]\n",
    "zipcode = aggregate(zipcode_pipeline)\n",
    "\n",
    "'''\n",
    "    The update_zipcode function cleans only two types of inconsistencies(TX 77086 and 77340-7856). \n",
    "    The below result shows that the two inconsistencies are cleaned very well. But, we do have other \n",
    "    invalid formats like street name in zipcode or a 9 digit zipcode without '-' or a zipcode with \n",
    "    less than 5 digits. If the zipcode auditing is very critical then we have to clean those kind \n",
    "    of inconsistencies by revisiting the cleaning process (Data Wrangling is an iterative proccess). \n",
    "'''\n",
    "\n",
    "\"Lets print few cleaned zipcodes. The zipcode list is very huge so printing only few of them\"\n",
    "for number,item in enumerate(zipcode):\n",
    "    if number%10 == 0:\n",
    "        print item.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Ideas for additional improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 1) Imrpovements for OSM forum: After reviewing the amenities in The Woodlands area I found that not all amenities are updated in openstreetmap data. It would have been more helpful if the data is near complete. Only 10% of The woodlands area amenities are reported. OSM should also standardize the tags that are to be present. For example, the phone number attribute in address tag should be made mandatory for all the eataries(Restaurants,Cafe,Bars etc..) \n",
    "\n",
    "##### 2) Project Improvements: In the initial phases of the project most of my time is  spent on parsing the huge OSM file many times. I believe iterative parsing is one of the efficient techniques to save the resources. Off late I have seen many articles on improving the efficeiency within python programming. For example, use of Psyco module, Threading in multiprocessor environment, Divide and Conquer. I would like to explore further on these different techniques to improve the efficiency. But, to implement these kind of techniques we should have extra knowledge on the opeating system of local machine. This woud be an extra overhead if the Data Analyst is not well versed with the OS concepts. If the system resources are very critical and the file we are operating is very large we may have to expolre any other optimizing techniques other than iterative parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### The main steps in data wrangling are covered in this project. As the data was updated by humans it is very definite that we will have errors. I have idnetified few inconsistencies and cleaned them. I am pretty sure that there will be many other errors. The data inserted in Mongo DB is not gold standard as the process of auditing and cleaning is done only once. In reality it is done iteratively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### References:\n",
    "[1] Data Wrangling course- Udacity\n",
    "\n",
    "[2] https://wiki.openstreetmap.org/wiki/Using_OpenStreetMap\n",
    "\n",
    "[3] https://wiki.openstreetmap.org/wiki/Map_Features\n",
    "\n",
    "[4] http://www.houstontx.gov/council/\n",
    "\n",
    "[5] https://stackoverflow.com/questions/20469712/using-and-with-match-in-mongodb\n",
    "\n",
    "[6] https://stackoverflow.com/questions/27272699/cant-get-allowdiskusetrue-to-work-with-pymongo\n",
    "\n",
    "[7] Udacity discussion forum [Main reference]\n",
    "\n",
    "[8] https://pypi.python.org/pypi/phonenumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
